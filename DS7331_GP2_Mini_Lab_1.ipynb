{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Scox97/DS7331_Group_Project/blob/main/DS7331_GP2_Mini_Lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUsB9XxPqX_G"
      },
      "source": [
        "# DS7331 Project: Phishing Dataset\n",
        "\n",
        "- Hayoung Cheon\n",
        "- Steven Cox\n",
        "- Erika Dupond\n",
        "- Miguel\n",
        "\n",
        "\n",
        "## Mini-Lab: Logistic Regression and SVMs\n",
        "\n",
        "You are to perform predictive analysis (classification) upon a data set: model the dataset using\n",
        "methods we have discussed in class: logistic regression and support vector machines, and making\n",
        "conclusions from the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vw8Hn3cBI45"
      },
      "source": [
        "### General Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRh1cv10qX_H"
      },
      "outputs": [],
      "source": [
        "# general\n",
        "import math\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# SKLearn\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        ")\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "TABLE_LAYOUT = [\n",
        "    {'selector': 'table', 'props': [('width', '100%')]},\n",
        "    {'selector': 'thead th', 'props': [('text-align', 'center')]},\n",
        "    {'selector': 'td', 'props': [('text-align', 'left')]}\n",
        "]\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HGHUJV9qX_I"
      },
      "outputs": [],
      "source": [
        "# URL to the dataset\n",
        "url = \"https://archive.ics.uci.edu/static/public/967/phiusiil+phishing+url+dataset.zip\"\n",
        "\n",
        "# Read the CSV file from the URL, ignoring the first column (index 0)\n",
        "df = pd.read_csv(url, encoding=\"utf-8\")\n",
        "df = df.drop(\"FILENAME\", axis=1)  # Column \"FILENAME\" can be ignored.\n",
        "print(\"\\nShape of data:\", df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z5dkgkuqX_I"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhRDcDMqqX_J"
      },
      "source": [
        "### Encoding and Finding High Cardinality Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz63av2QqX_J"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "\"\"\"\n",
        "# Optional Code: This is just to get the exercise going.\n",
        "\n",
        "- Since URL and others high cardinality, can cause the model not to scale.\n",
        "- In Addition, it is expensive computationally to produce such high cardinality features.\n",
        "- There is an opportunity to do a Transformation to continuous attribute\n",
        "\n",
        "Reference on:\n",
        "Julie Moeyersoms, David Martens\n",
        "Including high-cardinality attributes in predictive models: A case study in churn prediction in the energy sector, Decision Support Systems, Volume 72,2015\n",
        "Pages 72-81,\n",
        "ISSN 0167-9236\n",
        "https://www.sciencedirect.com/science/article/pii/S0167923615000275#s0120\n",
        "\n",
        "Hash Encoding:  https://towardsdatascience.com/4-ways-to-encode-categorical-features-with-high-cardinality-1bc6d8fd7b13/\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def shannon_entropy(text):\n",
        "    if not text or not isinstance(text, str):\n",
        "        return 0\n",
        "    counts = Counter(text)\n",
        "    probs = [freq / len(text) for freq in counts.values()]\n",
        "    return -sum(p * np.log2(p) for p in probs if p > 0)\n",
        "\n",
        "\n",
        "def pre_process_data(\n",
        "    input_df, drop_cols=[], use_entropy=True, include_aggregates=False\n",
        "):\n",
        "    df = input_df.copy()\n",
        "    y = df[\"label\"]\n",
        "\n",
        "    X = df.drop(columns=[col for col in drop_cols if col in df.columns] + [\"label\"])\n",
        "\n",
        "    binary_cols = X.nunique() == 2\n",
        "    for col in X.columns[binary_cols]:\n",
        "        X[col] = X[col].astype(int)\n",
        "\n",
        "    categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
        "    low_cardinality_cols = [col for col in categorical_cols if X[col].nunique() < 200]\n",
        "    high_cardinality_cols = [col for col in categorical_cols if X[col].nunique() >= 200]\n",
        "\n",
        "    if use_entropy:\n",
        "        for col in [\"URL\", \"Domain\", \"Title\"]:\n",
        "            if col in df.columns:\n",
        "                X[f\"{col}_entropy\"] = df[col].astype(str).apply(shannon_entropy)\n",
        "                print(f\"Entropy for {col}: {X[f'{col}_entropy'].mean():.4f}\")\n",
        "\n",
        "        X = X.drop(\n",
        "            columns=[col for col in high_cardinality_cols if col in X.columns],\n",
        "            errors=\"ignore\",\n",
        "        )\n",
        "    else:\n",
        "        hashed_dfs = []\n",
        "        for col in high_cardinality_cols:\n",
        "            unique_count = X[col].nunique()\n",
        "            n_features = (\n",
        "                10 if unique_count < 200 else min(max(unique_count // 10, 10), 500)\n",
        "            )\n",
        "            print(\n",
        "                f\"Hashing {col} with {n_features} features (from {unique_count} unique values)\"\n",
        "            )\n",
        "            hasher = FeatureHasher(n_features=n_features, input_type=\"string\")\n",
        "            hashed_features = hasher.fit_transform(\n",
        "                X[col].astype(str).values.reshape(-1, 1)\n",
        "            )\n",
        "            hashed_df = pd.DataFrame(\n",
        "                hashed_features.toarray(),\n",
        "                columns=[f\"{col}_hash_{i}\" for i in range(n_features)],\n",
        "            )\n",
        "            hashed_dfs.append(hashed_df)\n",
        "            X = X.drop(columns=[col])\n",
        "        if hashed_dfs:\n",
        "            hashed_combined = pd.concat(hashed_dfs, axis=1).reset_index(drop=True)\n",
        "            X = pd.concat([X.reset_index(drop=True), hashed_combined], axis=1)\n",
        "\n",
        "    for col in low_cardinality_cols:\n",
        "        dummies = pd.get_dummies(X[col], prefix=col, drop_first=True)\n",
        "        X = pd.concat([X.drop(columns=[col]), dummies], axis=1)\n",
        "        print(f\"Dummy encoding {col} with {dummies.shape[1]} features\")\n",
        "\n",
        "    if include_aggregates:\n",
        "        special_cols = [\n",
        "            \"NoOfEqualsInURL\",\n",
        "            \"NoOfQMarkInURL\",\n",
        "            \"NoOfAmpersandInURL\",\n",
        "            \"NoOfOtherSpecialCharsInURL\",\n",
        "        ]\n",
        "        resource_cols = [\"NoOfImage\", \"NoOfCSS\", \"NoOfJS\", \"NoOfExternalRef\"]\n",
        "        if all(col in df.columns for col in special_cols):\n",
        "            X[\"TotalSpecialChars\"] = df[special_cols].sum(axis=1)\n",
        "            print(\"Added TotalSpecialChars\")\n",
        "        if all(col in df.columns for col in resource_cols):\n",
        "            X[\"TotalExternalResources\"] = df[resource_cols].sum(axis=1)\n",
        "            print(\"Added TotalExternalResources\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def check_leakage(df, columns=[\"URL\", \"Title\"]):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    leakages = []\n",
        "    for i, col in enumerate(columns):\n",
        "        if col in df.columns:\n",
        "            unique_vals = df.groupby(col)[\"label\"].nunique()\n",
        "            leaks = (unique_vals == 1).sum()\n",
        "            total = len(unique_vals)\n",
        "            leak_pct = leaks / total\n",
        "\n",
        "        leakages.append(\n",
        "            {\n",
        "                \"feature\": col,\n",
        "                \"total\": total,\n",
        "                \"leaks\": leaks,\n",
        "                \"leak_pct\": float(leak_pct * 100),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        categories = [\"Leakage\", \"No Leakage\"]\n",
        "        values = [leaks, total - leaks]\n",
        "\n",
        "        bars = axes[i].bar(categories, values)\n",
        "        axes[i].set_title(f\"Data Leakage in {col} Column\")\n",
        "        axes[i].set_ylabel(\"Count of Unique Values\")\n",
        "\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            axes[i].annotate(\n",
        "                f\"{height}\",\n",
        "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 3),\n",
        "                textcoords=\"offset points\",\n",
        "                ha=\"center\",\n",
        "                va=\"bottom\",\n",
        "            )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    leakages = pd.DataFrame(leakages)\n",
        "    display(leakages)\n",
        "\n",
        "\n",
        "def pca_analysis(df):\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df)\n",
        "\n",
        "    pca = PCA().fit(X_scaled)\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "    plt.xlabel(\"Number of Components\")\n",
        "    plt.ylabel(\"Cumulative Explained Variance\")\n",
        "    plt.title(\"PCA: Explained Variance\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Validation Function\n",
        "def validate_holdout(X_train, X_holdout):\n",
        "    train_hashes = pd.util.hash_pandas_object(pd.DataFrame(X_train)).values\n",
        "    holdout_hashes = pd.util.hash_pandas_object(pd.DataFrame(X_holdout)).values\n",
        "    overlap = np.intersect1d(train_hashes, holdout_hashes)\n",
        "    print(f\"Number of overlapping samples: {len(overlap)}\")\n",
        "\n",
        "    y_pred = pipeline.predict(X_holdout)\n",
        "    cm = confusion_matrix(y_holdout, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "    accuracy = accuracy_score(y_holdout, y_pred)\n",
        "    precision = precision_score(y_holdout, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_holdout, y_pred, zero_division=0)\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
        "    f1 = f1_score(y_holdout, y_pred, zero_division=0)\n",
        "\n",
        "    metrics_df = pd.DataFrame({\n",
        "        \"Metric\": [\n",
        "            \"Accuracy\",\n",
        "            \"Precision (PPV)\",\n",
        "            \"Recall (Sensitivity)\",\n",
        "            \"Specificity (TNR)\",\n",
        "            \"Negative Predictive Value (NPV)\",\n",
        "            \"F1 Score\",\n",
        "        ],\n",
        "        \"Value\": [accuracy, precision, recall, specificity, npv, f1],\n",
        "    })\n",
        "\n",
        "    metrics_df[\"Value\"] = (metrics_df[\"Value\"] * 100).round(2).astype(str) + \"%\"\n",
        "    display(metrics_df)\n",
        "\n",
        "    conf_matrix_df = pd.DataFrame(\n",
        "        cm,\n",
        "        index=[\"Actual 0 (Non-phishing)\", \"Actual 1 (Phishing)\"],\n",
        "        columns=[\"Predicted 0 (Non-phishing)\", \"Predicted 1 (Phishing)\"]\n",
        "    )\n",
        "    display(conf_matrix_df)\n",
        "\n",
        "    labels = ['0', '1']\n",
        "    conf_mat = confusion_matrix(y_holdout, y_pred)\n",
        "\n",
        "    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t7lK4lcqX_J"
      },
      "source": [
        "#### PCA Analysis and Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGq_VaN2qX_K"
      },
      "outputs": [],
      "source": [
        "# PCA Check on preprocessed data\n",
        "\"\"\"\n",
        "Pre-process and encoding features. We are exploring those categorical columns looking and addressing:\n",
        "\n",
        "- High cardinality features and how with more advanced encoding (hashing) can improve the variance and reducing the feature space.\n",
        "\"\"\"\n",
        "X, y = pre_process_data(df, use_entropy=False)\n",
        "pca_analysis(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ga6dfMYqX_K"
      },
      "source": [
        "##### PCA Summary - Feature Space\n",
        "\n",
        "The feature space has 1619 features (original + hashed)\n",
        "\n",
        "###### Explained Variance Distribution\n",
        "\n",
        "- The cumulative explained variance curve increases linearly with the number of components.\n",
        "- About 1200 components are needed to explain ~80% of the total variance.\n",
        "- The first few components do not capture a dominant share of variance, suggesting:\n",
        "    - Our data is high-dimensional, with variance spread out across many features.\n",
        "    - Little redundancy is apparent; most features contribute independently to variance.\n",
        "\n",
        "###### Implications\n",
        "\n",
        "- Dimensionality reduction (PCA) won’t significantly condense information into a small number of components.\n",
        "- The hashing of high-cardinality features (e.g., URL, Title) spreads variance across many features, as each hashed bucket captures partial variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgBGI-PyqX_S"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression with Lasso with Hashed and Dummy encoded data - Warning this will take a while\n",
        "# scaler = StandardScaler()\n",
        "# X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     X_scaled, y, test_size=0.3, stratify=y, random_state=42\n",
        "# )\n",
        "\n",
        "# lasso = LogisticRegression(\n",
        "#     C=0.05,\n",
        "#     verbose=2,\n",
        "#     l1_ratio=0.5,\n",
        "#     max_iter=250,\n",
        "#     solver=\"saga\",\n",
        "#     penalty=\"elasticnet\",\n",
        "# )\n",
        "\n",
        "# lasso.fit(X_train, y_train)\n",
        "# y_pred_proba = lasso.predict_proba(X_test)[:, 1]\n",
        "# auc_lasso = roc_auc_score(y_test, y_pred_proba)\n",
        "# print(f\"Logistic Regression AUC: {auc_lasso:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKwKjKnTqX_S"
      },
      "source": [
        "##### Leakages summary\n",
        "\n",
        "\n",
        "- Initial Findings\n",
        "  - Engineered hashed features from `URL` and `Title`, those categorical features initially displayed significant signals (domain knowledge).\n",
        "\n",
        "- Discovery of Data Leakage\n",
        "  - Analysis showed:\n",
        "    - 100% of `URL`s correspond to a unique label – direct leakage.\n",
        "    - 99.9% of `Title`s correspond to a unique label.\n",
        "  - Hashing did not prevent leakage, as uniqueness was preserved in the hash.\n",
        "\n",
        "- Dimensionality & Redundancy Check\n",
        "  - PCA revealed:\n",
        "    - ~80% variance explained by ~1200 components of ~1600 total features.\n",
        "\n",
        "- Logistic Regression Modeling\n",
        "  - Trained models with scikit-learn and PyTorch (LASSO).\n",
        "  - Observed AUC=1.0 – clear overfitting due to leakage from raw `URL` and `Title`.\n",
        "\n",
        "- Key Decision: Feature Dropping\n",
        "  - Dropped `URL`, `Title`, and their hashed versions to eliminate data leakage.\n",
        "  - Retained engineered features (e.g., `URLLength`, `ObfuscationRatio`, `LetterRatioInURL`).\n",
        "- Recognized Logistic Regression’s limitations for high-dimensional, correlated data.\n",
        "\n",
        "- Learning & Next Steps\n",
        "  - Valuable lesson on how high-cardinality features can introduce leakage.\n",
        "  - What to do next:\n",
        "    - Focus on regularization, cross-validation, and robust validation strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-MKfxJTqX_T"
      },
      "outputs": [],
      "source": [
        "# Finding features that could be dropped\n",
        "def plot_relation_dist(ax, df, feature):\n",
        "    data_0 = df[df['label'] == 0][feature]\n",
        "    data_1 = df[df['label'] == 1][feature]\n",
        "\n",
        "    if data_0.std() > 0:\n",
        "        data_0.plot(kind='kde', ax=ax, label='Label 0 (Non-phishing)', alpha=0.5)\n",
        "\n",
        "    ax.hist(data_1, bins=1, range=(-0.5,0.5), density=True, alpha=0.5, label='Label 1 (Phishing)')\n",
        "\n",
        "    ax.set_xlim(-2, 2)\n",
        "    ax.set_xlabel(feature)\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.set_title(f'Distribution of {feature}')\n",
        "    ax.grid(alpha=0.3)\n",
        "    ax.legend()\n",
        "\n",
        "potential_drop = [\n",
        "    \"NoOfImage\",\n",
        "    \"ObfuscationRatio\",\n",
        "    \"NoOfObfuscatedChar\",\n",
        "    \"NoOfQMarkInURL\",\n",
        "    \"NoOfEqualsInURL\",\n",
        "    \"NoOfAmpersandInURL\",\n",
        "]\n",
        "\n",
        "n_features = len(potential_drop)\n",
        "n_cols = 3\n",
        "n_rows = (n_features + n_cols - 1) // n_cols\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(potential_drop):\n",
        "    plot_relation_dist(axes[i], df, feature)\n",
        "\n",
        "for j in range(i+1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "df[potential_drop].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LibRRok0qX_T"
      },
      "source": [
        "Noticed these features were not that useful; starting with stats analysis, the low mean and standard deviation (close to 0) raised suspicions. Visualizations confirmed these features could act as shortcuts for class separation, especially in Logistic Regression, leading to overfitting and poor generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el7ZkTZfqX_T"
      },
      "source": [
        "### Modeling Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQGJVBbfqX_T"
      },
      "source": [
        "#### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7uBL_QhqX_T"
      },
      "outputs": [],
      "source": [
        "## Feature Selection\n",
        "base_drop = [\n",
        "    \"URL\",\n",
        "    \"Domain\",\n",
        "    \"Title\",\n",
        "    \"TLD\",\n",
        "    \"URLSimilarityIndex\",\n",
        "    \"HasSocialNet\",\n",
        "    \"HasCopyrightInfo\",\n",
        "    \"HasTitle\",\n",
        "    \"IsHTTPS\",\n",
        "    \"IsResponsive\",\n",
        "    \"IsDomainIP\",\n",
        "    \"HasDescription\",\n",
        "    \"HasObfuscation\",\n",
        "    \"HasSubmitButton\",\n",
        "    \"HasExternalSubmitForm\",\n",
        "]\n",
        "potential_drop = [\n",
        "    \"NoOfImage\",\n",
        "    \"ObfuscationRatio\",\n",
        "    \"NoOfObfuscatedChar\",\n",
        "    \"NoOfQMarkInURL\",\n",
        "    \"NoOfEqualsInURL\",\n",
        "    \"NoOfAmpersandInURL\",\n",
        "]\n",
        "drop = base_drop + potential_drop\n",
        "\n",
        "X_final, y_final = pre_process_data(\n",
        "    df, drop_cols=drop, use_entropy=False, include_aggregates=False\n",
        ")\n",
        "\n",
        "continuous_features = [col for col in X_final.columns if X_final[col].nunique() > 2]\n",
        "\n",
        "skewness = X_final[continuous_features].skew().sort_values(ascending=False)\n",
        "print(f\"\\nFeatures with high skewness:\")\n",
        "print(skewness)\n",
        "\n",
        "low_variance = X_final[continuous_features].std().sort_values()\n",
        "print(f\"\\n=== Features with low variance:\")\n",
        "print(low_variance[low_variance < 0.1])\n",
        "\n",
        "X_final[continuous_features].hist(bins=50, figsize=(20,15))\n",
        "plt.suptitle(\"\\n=== Feature Distributions\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-01T20:51:55.374933Z",
          "start_time": "2025-06-01T20:51:46.773553Z"
        },
        "id": "uEoAaS9fqX_T"
      },
      "outputs": [],
      "source": [
        "## Feature Analysis\n",
        "numerical_cols = [\n",
        "    col for col in X_final.select_dtypes(include=['float64', 'int64'])\n",
        "    if X_final[col].nunique() > 2\n",
        "]\n",
        "\n",
        "num_cols = 5\n",
        "num_rows = int(np.ceil(len(numerical_cols) / num_cols))\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(5*num_cols, 4*num_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "y_aligned = y_final.reset_index(drop=True)\n",
        "X_aligned = X_final.reset_index(drop=True)\n",
        "\n",
        "for i, feature in enumerate(numerical_cols):\n",
        "    ax = axes[i]\n",
        "    data_non_phishing = X_aligned[feature][y_aligned == 0].dropna()\n",
        "    data_phishing = X_aligned[feature][y_aligned == 1].dropna()\n",
        "\n",
        "    if data_non_phishing.nunique() > 1:\n",
        "        sns.kdeplot(data_non_phishing, fill=True, color='skyblue', label='Non-Phishing', alpha=0.5, ax=ax)\n",
        "    if data_phishing.nunique() > 1:\n",
        "        sns.kdeplot(data_phishing, fill=True, color='salmon', label='Phishing', alpha=0.5, ax=ax)\n",
        "\n",
        "    ax.set_title(f\"{feature} by Target\")\n",
        "    ax.set_xlabel(feature)\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.legend()\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "for j in range(len(numerical_cols), len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout(pad=2.0)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "df_temp = X_final.copy()\n",
        "df_temp['label'] = y_final\n",
        "corr_pearson = df_temp.corr(method='pearson')['label'].sort_values(ascending=False)\n",
        "corr_spearman = df_temp.corr(method='spearman')['label'].sort_values(ascending=False)\n",
        "\n",
        "corr_df = pd.DataFrame({'Pearson': corr_pearson, 'Spearman': corr_spearman}).drop(index='label')\n",
        "print(\"\\nCorrelations with Target:\\n\")\n",
        "display(corr_df.sort_values(by='Pearson', key=abs, ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-01T20:55:21.097761Z",
          "start_time": "2025-06-01T20:55:19.851535Z"
        },
        "id": "uwlqDLy4qX_U"
      },
      "outputs": [],
      "source": [
        "## Feature Analysis\n",
        "\n",
        "feature_correlations = X_final.copy()\n",
        "feature_correlations[\"label\"] = y_final\n",
        "corrs = feature_correlations.corr()[\"label\"].sort_values(ascending=False)\n",
        "\n",
        "binary_features = []\n",
        "for col in X_final.columns:\n",
        "    unique_vals = X_final[col].nunique()\n",
        "    if unique_vals == 2:\n",
        "        binary_features.append(col)\n",
        "\n",
        "num_cols = 6\n",
        "total_plots = 1 + len(binary_features)\n",
        "num_rows = math.ceil(total_plots / num_cols)\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(28, 5 * num_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "top_n = 15\n",
        "top_corrs = corrs.iloc[:top_n]\n",
        "bottom_corrs = corrs.iloc[-top_n:] if len(corrs) > top_n else pd.Series()\n",
        "plot_corrs = pd.concat([top_corrs, bottom_corrs])\n",
        "colors = [\"#1e88e5\" if c > 0 else \"#ff0d57\" for c in plot_corrs]\n",
        "\n",
        "ax0 = axes[0]\n",
        "plot_corrs.plot(kind=\"barh\", ax=ax0, color=colors)\n",
        "ax0.set_title(\"Top Feature Correlations with Target\", fontsize=12)\n",
        "ax0.axvline(x=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
        "ax0.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
        "ax0.set_xlabel(\"Correlation Coefficient\")\n",
        "\n",
        "for i, feature in enumerate(binary_features):\n",
        "    if i + 1 < len(axes):\n",
        "        ax = axes[i + 1]\n",
        "        ct = pd.crosstab(X_final[feature], y_final, normalize=\"index\") * 100\n",
        "\n",
        "        ct.plot(kind=\"bar\", stacked=True, ax=ax)\n",
        "\n",
        "        for c in ax.containers:\n",
        "            ax.bar_label(c, fmt=\"%.1f%%\")\n",
        "\n",
        "        ax.set_title(f\"{feature} vs Target\", fontsize=12)\n",
        "        ax.set_ylabel(\"Percentage\")\n",
        "        ax.set_xlabel(\"Feature Value\")\n",
        "        ax.legend(title=\"Target\", labels=[\"Non-Phishing (0)\", \"Phishing (1)\"])\n",
        "\n",
        "        ct_raw = pd.crosstab(X_final[feature], y_final)\n",
        "        total = ct_raw.sum().sum()\n",
        "        perfect_split = (ct_raw == 0).any().any()\n",
        "\n",
        "        if perfect_split:\n",
        "            ax.text(\n",
        "                0.5,\n",
        "                -0.15,\n",
        "                \"Potential data leakage detected!\",\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"red\",\n",
        "                transform=ax.transAxes,\n",
        "                fontsize=11,\n",
        "            )\n",
        "\n",
        "for i in range(total_plots, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout(pad=2.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crAV1BLkqX_U"
      },
      "source": [
        "After deep dive after looking those features where mean and std deviation looked suspicious, we decided to deep dive on the features for skewness and low variance. Those were candidates to log transform to have a more constant variance that makes more appropriate for Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNaBVWkeqX_U"
      },
      "source": [
        "#### Logistic Regression Modeling\n",
        "\n",
        "For the modeling phase we decided to create a pipeline where we transform those candidate features into log space and then standardize the rest of the features. Seemed appropriate after multiple runs to expose the model to a new data by creating a holdout set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ7tccn4qX_U"
      },
      "outputs": [],
      "source": [
        "## Logistic Regression\n",
        "high_skew_features = [\n",
        "    \"NoOfCSS\",\n",
        "    \"NoOfJS\",\n",
        "    \"NoOfEmptyRef\",\n",
        "    \"NoOfiFrame\",\n",
        "    \"NoOfDegitsInURL\",\n",
        "    \"NoOfPopup\",\n",
        "    \"NoOfExternalRef\",\n",
        "    \"NoOfSelfRef\",\n",
        "    \"NoOfLettersInURL\",\n",
        "    \"URLLength\",\n",
        "    \"LineOfCode\",\n",
        "    \"LargestLineLength\",\n",
        "    \"NoOfOtherSpecialCharsInURL\",\n",
        "    \"DegitRatioInURL\",\n",
        "    \"DomainLength\",\n",
        "    \"NoOfSubDomain\",\n",
        "    \"TLDLength\",\n",
        "    \"SpacialCharRatioInURL\",\n",
        "]\n",
        "standard_scale_features = [\n",
        "    col for col in continuous_features if col not in high_skew_features\n",
        "]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"log\", FunctionTransformer(np.log1p, validate=True), high_skew_features),\n",
        "        (\"scale\", StandardScaler(), standard_scale_features),\n",
        "    ],\n",
        "    remainder=\"passthrough\",\n",
        ")\n",
        "\n",
        "lasso = LogisticRegression(\n",
        "    penalty=\"l1\",\n",
        "    solver=\"saga\",\n",
        "    C=0.0001,\n",
        "    max_iter=5000,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"lasso\", lasso)])\n",
        "\n",
        "print(\"\\nRunning hold-out set evaluation...\")\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
        "    X_final, y_final, test_size=0.2, random_state=42, stratify=y_final\n",
        ")\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_proba = pipeline.predict_proba(X_holdout)[:, 1]\n",
        "y_pred = pipeline.predict(X_holdout)\n",
        "\n",
        "holdout_auc = roc_auc_score(y_holdout, y_pred_proba)\n",
        "holdout_acc = accuracy_score(y_holdout, y_pred)\n",
        "holdout_f1 = f1_score(y_holdout, y_pred)\n",
        "\n",
        "print(f\"\\nHold-out AUC: {holdout_auc:.3f}\")\n",
        "print(f\"Hold-out Accuracy: {holdout_acc:.3f}\")\n",
        "print(f\"Hold-out F1 Score: {holdout_f1:.3f}\")\n",
        "\n",
        "validate_holdout(X_train, X_holdout)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLr-vYOwqX_U"
      },
      "source": [
        "### SVM - Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KRFgY91qX_V"
      },
      "outputs": [],
      "source": [
        "## SVM\n",
        "to_scale = standard_scale_features + high_skew_features\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"scale\", StandardScaler(), to_scale),\n",
        "    ],\n",
        "    remainder=\"passthrough\",\n",
        ")\n",
        "\n",
        "sdg = SGDClassifier(loss='hinge', alpha=0.0001, penalty='l1', random_state=42)\n",
        "pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"sdg\", sdg)])\n",
        "\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
        "    X_final, y_final, test_size=0.2, random_state=42, stratify=y_final\n",
        ")\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_decision = pipeline.decision_function(X_holdout)\n",
        "y_pred = pipeline.predict(X_holdout)\n",
        "\n",
        "holdout_auc = roc_auc_score(y_holdout, y_decision)\n",
        "holdout_acc = accuracy_score(y_holdout, y_pred)\n",
        "holdout_f1 = f1_score(y_holdout, y_pred)\n",
        "\n",
        "print(f\"\\nHold-out AUC: {holdout_auc:.3f}\")\n",
        "print(f\"Hold-out Accuracy: {holdout_acc:.3f}\")\n",
        "print(f\"Hold-out F1 Score: {holdout_f1:.3f}\")\n",
        "\n",
        "validate_holdout(X_train, X_holdout)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "phishing-0NKtdish-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}